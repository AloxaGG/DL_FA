{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKMq7dp2W15Y",
        "outputId": "af55cb50-639f-48cd-86b8-32180e913bb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Генерирование русских имен при помощи RNN"
      ],
      "metadata": {
        "collapsed": false,
        "id": "VVbt1HRvIo34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Датасет: https://disk.yandex.ru/i/2yt18jHUgVEoIw\n",
        "\n",
        "1.1 На основе файла name_rus.txt создайте датасет.\n",
        "  * Учтите, что имена могут иметь различную длину\n",
        "  * Добавьте 4 специальных токена:\n",
        "    * `<PAD>` для дополнения последовательности до нужной длины;\n",
        "    * `<UNK>` для корректной обработки ранее не встречавшихся токенов;\n",
        "    * `<SOS>` для обозначения начала последовательности;\n",
        "    * `<EOS>` для обозначения конца последовательности.\n",
        "  * Преобразовывайте строку в последовательность индексов с учетом следующих замечаний:\n",
        "    * в начало последовательности добавьте токен `<SOS>`;\n",
        "    * в конец последовательности добавьте токен `<EOS>` и, при необходимости, несколько токенов `<PAD>`;\n",
        "  * `Dataset.__get_item__` возращает две последовательности: последовательность для обучения и правильный ответ.\n",
        "\n",
        "  Пример:\n",
        "  ```\n",
        "  s = 'The cat sat on the mat'\n",
        "  # преобразуем в индексы\n",
        "  s_idx = [2, 5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
        "  # получаем x и y (__getitem__)\n",
        "  x = [2, 5, 1, 2, 8, 4, 7, 3, 0]\n",
        "  y = [5, 1, 2, 8, 4, 7, 3, 0, 0]\n",
        "  ```\n",
        "\n",
        "1.2 Создайте и обучите модель для генерации фамилии.\n",
        "\n",
        "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`;\n",
        "  * Используйте рекуррентные слои;\n",
        "  * Задача ставится как предсказание следующего токена в каждом примере из пакета для каждого момента времени. Т.е. в данный момент времени по текущей подстроке предсказывает следующий символ для данной строки (задача классификации);\n",
        "  * Примерная схема реализации метода `forward`:\n",
        "  ```\n",
        "    input_X: [batch_size x seq_len] -> nn.Embedding -> emb_X: [batch_size x seq_len x embedding_size]\n",
        "    emb_X: [batch_size x seq_len x embedding_size] -> nn.RNN -> output: [batch_size x seq_len x hidden_size]\n",
        "    output: [batch_size x seq_len x hidden_size] -> torch.Tensor.reshape -> output: [batch_size * seq_len x hidden_size]\n",
        "    output: [batch_size * seq_len x hidden_size] -> nn.Linear -> output: [batch_size * seq_len x vocab_size]\n",
        "  ```\n",
        "\n",
        "1.3 Напишите функцию, которая генерирует фамилию при помощи обученной модели:\n",
        "  * Построение начинается с последовательности единичной длины, состоящей из индекса токена `<SOS>`;\n",
        "  * Начальное скрытое состояние RNN `h_t = None`;\n",
        "  * В результате прогона последнего токена из построенной последовательности через модель получаете новое скрытое состояние `h_t` и распределение над всеми токенами из словаря;\n",
        "  * Выбираете 1 токен пропорционально вероятности и добавляете его в последовательность (можно воспользоваться `torch.multinomial`);\n",
        "  * Повторяете эти действия до тех пор, пока не сгенерирован токен `<EOS>` или не превышена максимальная длина последовательности.\n",
        "\n",
        "При обучении каждые `k` эпох генерируйте несколько фамилий и выводите их на экран."
      ],
      "metadata": {
        "collapsed": false,
        "id": "HyS1zuHQIo36"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      surname\n",
            "0     авдокея\n",
            "1     авдоким\n",
            "2      авдоня\n",
            "3    авдотька\n",
            "4  авдотьюшка\n",
            "\n",
            "[12 20 11 23 18  0]\n",
            "лукцса\n",
            "[34, 12, 20, 11, 23, 18, 0, 33, 33, 33, 33, 33, 33, 33, 35]\n",
            "лукцса\n"
          ]
        }
      ],
      "source": [
        "# 1.1\n",
        "data = pd.read_csv('name_rus.txt', encoding='cp1251', header=None, names=['surname'])\n",
        "print(data.head())\n",
        "\n",
        "\n",
        "class Vocab():\n",
        "    def __init__(self, _data):\n",
        "        self.max_surname_len = _data.surname.str.len().max()\n",
        "        self.token_to_id = {}\n",
        "        self.id_to_token = {}\n",
        "        self.tech = ['<PAD>', '<SOS>', '<EOS>', '<UNK>']\n",
        "        self.build_vocab(list('абвгдеёжзийклмнопрстуфхцчшщъыьэюя'))\n",
        "        self.vocab_size = len(self.token_to_id)\n",
        "\n",
        "    def build_vocab(self, letters):\n",
        "        self.token_to_id = {token: idx for idx, token in enumerate(letters + self.tech)}\n",
        "        self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.vocab_size\n",
        "\n",
        "    def __getitem__(self, token):\n",
        "        return self.token_to_id[token]\n",
        "\n",
        "    def __contains__(self, token):\n",
        "        return token in self.token_to_id\n",
        "\n",
        "    def to_tokens(self, ids):\n",
        "        # объединим id в одну строку\n",
        "        ids_sub = []\n",
        "        for idx in ids:\n",
        "            if self.id_to_token[int(idx)] not in self.tech:\n",
        "                ids_sub.append(self.id_to_token[int(idx)])\n",
        "\n",
        "        return ''.join(ids_sub)\n",
        "\n",
        "    def to_ids(self, tokens):\n",
        "        out = [self.token_to_id['<SOS>']] + [self.token_to_id['<PAD>']] * self.max_surname_len\n",
        "        for i, token in enumerate(tokens, 1):\n",
        "            if token not in self.token_to_id:\n",
        "                out[i] = self.token_to_id['<UNK>']\n",
        "            else:\n",
        "                out[i] = self.token_to_id[token]\n",
        "        out.append(self.token_to_id['<EOS>'])\n",
        "        return out\n",
        "\n",
        "\n",
        "vocab = Vocab(data)\n",
        "np.random.seed(21 * 3)\n",
        "\n",
        "test_tokens = np.random.randint(0, 32, 6)\n",
        "print('\\n', test_tokens, sep='')\n",
        "print(vocab.to_tokens(test_tokens))\n",
        "print(vocab.to_ids(vocab.to_tokens(test_tokens)))\n",
        "print(vocab.to_tokens((vocab.to_ids(vocab.to_tokens(test_tokens)))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SdTnDhvIo37",
        "outputId": "82db4f31-ab4c-4a4d-b3aa-7a3f0a6bbe40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftKEdDA4upDP",
        "outputId": "a726b783-456e-42c1-b8b5-3a57fa2c8198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12, 20, 11, 23, 18,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([34,  0,  2,  4, 15, 11,  5, 32, 33, 33, 33, 33, 33, 33]) tensor([ 0,  2,  4, 15, 11,  5, 32, 33, 33, 33, 33, 33, 33, 35])\n",
            "авдокея\n",
            "авдокея\n"
          ]
        }
      ],
      "source": [
        "class SurnameDataset(Dataset):\n",
        "\n",
        "    def __init__(self, _data, _vocab):\n",
        "        self.data = _data\n",
        "        self.vocab = _vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        _x = self.vocab.to_ids(self.data.surname.iloc[idx])\n",
        "        return torch.tensor(_x[:-1]), torch.tensor(_x[1:])\n",
        "\n",
        "\n",
        "dataset = SurnameDataset(data, vocab)\n",
        "x, y = next(iter(dataset))\n",
        "print(x, y)\n",
        "print(vocab.to_tokens(x))\n",
        "print(vocab.to_tokens(y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eja8YjTVIo38",
        "outputId": "d80ef7ef-6c66-4636-d137-c0b1a450e12b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "class MyModelRNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, _vocab: Vocab, embedding_size, hidden_size):\n",
        "        super(MyModelRNN, self).__init__()\n",
        "        self.embedding = torch.nn.Embedding(_vocab.vocab_size, embedding_size)\n",
        "        self.rnn = torch.nn.RNN(embedding_size, hidden_size, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_size, 1024)\n",
        "        self.fc2 = torch.nn.Linear(1024, _vocab.vocab_size)\n",
        "        self.f = torch.nn.Sigmoid()\n",
        "        self.dropout = torch.nn.Dropout(0.25)\n",
        "        self.vocab = _vocab\n",
        "\n",
        "    def forward(self, x, h=None):\n",
        "        x = self.embedding(x)\n",
        "        x, h = self.rnn(x, h)\n",
        "        x = self.dropout(self.fc(x))\n",
        "        x = self.fc2(self.f(x))\n",
        "        return x, h"
      ],
      "metadata": {
        "id": "3N654o5TIo39"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[ 0.1109,  0.7914, -0.0430,  ...,  0.0969,  0.6286,  0.1847],\n",
            "         [ 0.0342,  0.7987,  0.0149,  ...,  0.0975,  0.5195,  0.1857],\n",
            "         [ 0.0882,  0.8167,  0.0483,  ...,  0.0119,  0.5667,  0.1631],\n",
            "         ...,\n",
            "         [ 0.0538,  0.7993, -0.0260,  ..., -0.0667,  0.5354,  0.1516],\n",
            "         [ 0.0469,  0.7862, -0.0269,  ...,  0.0017,  0.5860,  0.1439],\n",
            "         [ 0.1976,  0.8402, -0.0628,  ...,  0.0577,  0.4790,  0.2603]],\n",
            "\n",
            "        [[ 0.0653,  0.7593, -0.0293,  ...,  0.1302,  0.5845,  0.1603],\n",
            "         [ 0.0557,  0.7981,  0.1300,  ..., -0.0087,  0.6054,  0.0569],\n",
            "         [ 0.1592,  0.7599, -0.0496,  ...,  0.0597,  0.5202,  0.1930],\n",
            "         ...,\n",
            "         [ 0.0810,  0.7915, -0.0622,  ..., -0.0100,  0.5935,  0.1392],\n",
            "         [ 0.0582,  0.8468, -0.0331,  ..., -0.0359,  0.5382,  0.1603],\n",
            "         [ 0.1395,  0.8935, -0.0528,  ...,  0.0451,  0.5273,  0.1830]]],\n",
            "       grad_fn=<ViewBackward0>), tensor([[[-0.8207,  0.5053,  0.9007,  0.1456, -0.8592,  0.2377, -0.5917,\n",
            "          -0.7038,  0.9767,  0.8574,  0.5245,  0.2087,  0.8530, -0.2189,\n",
            "           0.6280, -0.3920,  0.6776, -0.8687,  0.9463,  0.9130,  0.5301,\n",
            "          -0.7819, -0.0801, -0.9666,  0.2417, -0.5201,  0.9789, -0.9642,\n",
            "          -0.8084,  0.7276,  0.5720, -0.0753,  0.9421,  0.7886,  0.5352,\n",
            "           0.6029,  0.2276, -0.4518, -0.1339, -0.4754, -0.9525,  0.8158,\n",
            "           0.1302, -0.1590, -0.2519,  0.9374,  0.3741, -0.8590, -0.8322,\n",
            "          -0.8496,  0.3765, -0.6285,  0.4211, -0.9634,  0.3223, -0.1531,\n",
            "           0.3716,  0.3027, -0.8520, -0.0800, -0.5094,  0.5134,  0.8802,\n",
            "           0.9720,  0.3380,  0.4378, -0.9390,  0.8700,  0.8407, -0.8159,\n",
            "           0.3381,  0.0112,  0.5367,  0.0046, -0.9022,  0.7203,  0.4011,\n",
            "           0.4760,  0.5306,  0.4892, -0.3466,  0.8271, -0.6413,  0.9023,\n",
            "          -0.3405, -0.9262, -0.0701,  0.3766, -0.6040, -0.3450, -0.6975,\n",
            "          -0.0336, -0.5020,  0.9358, -0.8784,  0.2735, -0.1974,  0.4652,\n",
            "           0.9279,  0.6567, -0.1831,  0.9445, -0.4679,  0.6990, -0.2529,\n",
            "           0.1350, -0.5286, -0.2690,  0.1862,  0.4239,  0.0599,  0.5808,\n",
            "          -0.1854, -0.5600, -0.3904, -0.1008,  0.3312, -0.0192, -0.6261,\n",
            "           0.8480,  0.8271,  0.8461,  0.9456, -0.2846,  0.6807,  0.2978,\n",
            "          -0.1460, -0.1669],\n",
            "         [-0.8212,  0.5031,  0.9008,  0.1469, -0.8593,  0.2344, -0.5914,\n",
            "          -0.7031,  0.9766,  0.8578,  0.5252,  0.2081,  0.8536, -0.2181,\n",
            "           0.6271, -0.3933,  0.6764, -0.8690,  0.9463,  0.9128,  0.5303,\n",
            "          -0.7816, -0.0808, -0.9665,  0.2418, -0.5203,  0.9789, -0.9643,\n",
            "          -0.8082,  0.7278,  0.5724, -0.0765,  0.9423,  0.7885,  0.5323,\n",
            "           0.6030,  0.2265, -0.4526, -0.1364, -0.4763, -0.9525,  0.8168,\n",
            "           0.1318, -0.1622, -0.2501,  0.9374,  0.3753, -0.8589, -0.8329,\n",
            "          -0.8498,  0.3763, -0.6280,  0.4223, -0.9634,  0.3224, -0.1512,\n",
            "           0.3709,  0.3034, -0.8518, -0.0786, -0.5087,  0.5145,  0.8800,\n",
            "           0.9720,  0.3367,  0.4374, -0.9389,  0.8698,  0.8404, -0.8163,\n",
            "           0.3378,  0.0143,  0.5356,  0.0046, -0.9024,  0.7198,  0.4021,\n",
            "           0.4752,  0.5308,  0.4894, -0.3475,  0.8264, -0.6419,  0.9023,\n",
            "          -0.3409, -0.9261, -0.0699,  0.3758, -0.6033, -0.3460, -0.6983,\n",
            "          -0.0329, -0.5002,  0.9355, -0.8777,  0.2714, -0.1969,  0.4638,\n",
            "           0.9279,  0.6577, -0.1838,  0.9450, -0.4690,  0.6996, -0.2528,\n",
            "           0.1355, -0.5287, -0.2703,  0.1866,  0.4253,  0.0604,  0.5809,\n",
            "          -0.1872, -0.5597, -0.3918, -0.1019,  0.3311, -0.0195, -0.6274,\n",
            "           0.8485,  0.8261,  0.8458,  0.9456, -0.2846,  0.6819,  0.2982,\n",
            "          -0.1478, -0.1658]]], grad_fn=<StackBackward0>))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 15, 37])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "model = MyModelRNN(vocab, 256, 128)\n",
        "print(model(torch.tensor([vocab.to_ids(name) for name in ['виктор', 'егор']])))\n",
        "model(torch.tensor([vocab.to_ids(name) for name in ['виктор', 'егор']]))[0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tm4ikplIo39",
        "outputId": "6afac020-1507-4ecd-b3d0-122eecfa84fd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'в'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def sample_next(model, x, prev_state, topk=5, uniform=True):\n",
        "    out, state = model(x, prev_state)\n",
        "    last_out = out[0, -1, :]\n",
        "    topk = topk if topk else last_out.shape[0]\n",
        "    top_logit, top_ix = torch.topk(last_out, k=topk, dim=-1)\n",
        "    p = None if uniform else torch.nn.functional.softmax(top_logit.detach(), dim=-1).numpy()\n",
        "    sampled_ix = np.random.choice(top_ix, p=p)\n",
        "    return sampled_ix, state\n",
        "\n",
        "\n",
        "def sample(model, start_letters, topk=5, uniform=False, max_seqlen=15, stop_on=None):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sampled_ix_list = start_letters[:]\n",
        "        x = torch.tensor([start_letters])\n",
        "\n",
        "        prev_state = None\n",
        "        for t in range(max_seqlen - len(start_letters)):\n",
        "            sampled_ix, prev_state = sample_next(model, x, prev_state, topk, uniform)\n",
        "\n",
        "            sampled_ix_list.append(sampled_ix)\n",
        "            x = torch.tensor([[sampled_ix]])\n",
        "\n",
        "            if sampled_ix == stop_on:\n",
        "                break\n",
        "\n",
        "    model.train()\n",
        "    return sampled_ix_list\n",
        "\n",
        "\n",
        "vocab.to_tokens(sample(model, [2], stop_on=vocab.token_to_id['<EOS>']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HnDWScyRIo3-",
        "outputId": "3d426ffb-884d-4c88-c48b-481e49845cc1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"d6e32372-68dd-4428-a4b9-96b5e5e5364e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d6e32372-68dd-4428-a4b9-96b5e5e5364e\")) {                    Plotly.newPlot(                        \"d6e32372-68dd-4428-a4b9-96b5e5e5364e\",                        [{\"hovertemplate\":\"variable=train loss\\u003cbr\\u003eindex=%{x}\\u003cbr\\u003evalue=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"train loss\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"train loss\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75],\"xaxis\":\"x\",\"y\":[3.9015636444091797,2.720300316810608,2.5535680452982583,2.330777883529663,2.2297948201497397,2.129266301790873,2.0817126035690308,2.0271892150243125,1.976967453956604,1.9363755385080974,1.9014121691385906,1.8658591906229656,1.837581197420756,1.8096171220143635,1.7835286060969036,1.7578592697779338,1.7318158149719238,1.7071514924367268,1.6841594378153484,1.6622289419174194,1.653520981470744,1.623313546180725,1.603947639465332,1.596009651819865,1.5769456227620442,1.5645689566930134,1.5530187288920085,1.5373822053273518,1.5285909175872803,1.5142179330190022,1.4918965498606365,1.476180871327718,1.4965750773747761,1.4757473071416218,1.4642496903737385,1.500224232673645,1.5039348602294922,1.4767932891845703,1.46416970094045,1.419966419537862,1.3959647218386333,1.3902733723322551,1.3720658421516418,1.3656404415766399,1.40202134847641,1.4122292002042134,1.3895832101504009,1.4499321778615315,1.4285985231399536,1.404004454612732,1.3830487926801045,1.3413838744163513,1.3419097264607747,1.384933332602183,1.4222105344136555,1.4234214226404827,1.388713498910268,1.3666579723358154,1.3462464412053425,1.3283335169156392,1.3158379395802815,1.3065465887387593,1.2946471571922302,1.2842724124590557,1.2750985821088154,1.2710033853848774,1.264233112335205,1.2616765300432842,1.2571773926417034,1.265745997428894,1.27597572406133,1.2851417660713196,1.4051493803660076,1.575268030166626,1.3686312834421794,1.3660661776860554],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d6e32372-68dd-4428-a4b9-96b5e5e5364e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "px.line(pd.DataFrame({'train loss': loss_log}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "wK7eMcZbIo3_",
        "outputId": "edfa8271-2bbc-44f8-ef21-0bc8b6bbee21"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPOCH:   1 \t LOSS: 3.9016\n",
            "Викт --->  | виктаа | виктаа | виктаи\n",
            "EPOCH:  26 \t LOSS: 1.5646\n",
            "Викт --->  | виктита | викта | викта\n",
            "EPOCH:  51 \t LOSS: 1.3830\n",
            "Викт --->  | виктава | виктина | виктя\n",
            "EPOCH:  76 \t LOSS: 1.3661\n",
            "Викт --->  | виктья | виктья | виктичнич\n"
          ]
        }
      ],
      "source": [
        "def train(_model: torch.nn.Module, epochs=100):\n",
        "    optimizer = torch.optim.Adam(_model.parameters())\n",
        "    loss = torch.nn.CrossEntropyLoss()\n",
        "    loss_log = []\n",
        "    loader = DataLoader(dataset, batch_size=512)\n",
        "    for i in range(epochs):\n",
        "        epoch_loss = 0\n",
        "        j = 1  # Делители running losses\n",
        "        _model.train()\n",
        "        for j, (batch_x, batch_y) in enumerate(loader):\n",
        "            y_pred = _model(batch_x)\n",
        "            running_loss = loss(y_pred[0].reshape(-1, vocab.vocab_size), batch_y.reshape(-1))\n",
        "            epoch_loss += running_loss.item()\n",
        "\n",
        "            running_loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        _model.eval()\n",
        "        epoch_loss /= j\n",
        "\n",
        "        if i % 25 == 0:\n",
        "            print(f'EPOCH: {i + 1:3d} \\t LOSS: {epoch_loss:0.4f}')\n",
        "\n",
        "            eos = vocab.token_to_id['<EOS>']\n",
        "            start = vocab.to_ids('викт')[1:5]\n",
        "            samples = [vocab.to_tokens(sample(model, start, stop_on=eos)) for _ in range(3)]\n",
        "            print('Викт ---> ', *samples, sep=' | ')\n",
        "\n",
        "        loss_log.append(epoch_loss)\n",
        "\n",
        "    return _model, loss_log\n",
        "\n",
        "\n",
        "model, loss_log = train(model, epochs=76)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ5zf6BpIo3-",
        "outputId": "d4650d49-e28f-4f85-f790-d592a93fd548"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Викт --->  | виктия | викта | викта | викта | виктич | викта | виктия | викта | викта | викта\n"
          ]
        }
      ],
      "source": [
        "eos = vocab.token_to_id['<EOS>']\n",
        "start = vocab.to_ids('викт')[1:5]\n",
        "samples = [vocab.to_tokens(sample(model, start, stop_on=eos)) for _ in range(10)]\n",
        "print('Викт ---> ', *samples, sep=' | ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TATZB13Io4A",
        "outputId": "fffb0e30-c95c-45cd-99f0-7244a26b5f70"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJf5iaA2fOTM"
      },
      "source": [
        "## 2. Генерирование текста при помощи RNN\n",
        "\n",
        "2.1 Скачайте из интернета какое-нибудь художественное произведение\n",
        "  * Выбирайте достаточно крупное произведение, чтобы модель лучше обучалась;\n",
        "\n",
        "2.2 На основе выбранного произведения создайте датасет.\n",
        "\n",
        "Отличия от задачи 1:\n",
        "  * Токены <SOS>, `<EOS>` и `<UNK>` можно не добавлять;\n",
        "  * При создании датасета текст необходимо предварительно разбить на части. Выберите желаемую длину последовательности `seq_len` и разбейте текст на построки длины `seq_len` (можно без перекрытия, можно с небольшим перекрытием).\n",
        "\n",
        "2.3 Создайте и обучите модель для генерации текста\n",
        "  * Задача ставится точно так же как в 1.2;\n",
        "  * При необходимости можете применить:\n",
        "    * двухуровневые рекуррентные слои (`num_layers`=2)\n",
        "    * [обрезку градиентов](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)\n",
        "\n",
        "2.4 Напишите функцию, которая генерирует фрагмент текста при помощи обученной модели\n",
        "  * Процесс генерации начинается с небольшого фрагмента текста `prime`, выбранного вами (1-2 слова)\n",
        "  * Сначала вы пропускаете через модель токены из `prime` и генерируете на их основе скрытое состояние рекуррентного слоя `h_t`;\n",
        "  * После этого вы генерируете строку нужной длины аналогично 1.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chapter', 'i', 'in']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "with open('The-Great-Gatsby.txt') as file:\n",
        "    text = file.read()\n",
        "\n",
        "text = nltk.word_tokenize(re.sub(r'[^A-Z]', '', text.lower(), -1), 'english')\n",
        "text[:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OfypMRsIo4B",
        "outputId": "01accfd0-2abb-4360-f3d4-05aa8c17bb0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 68.0098\n",
            "  bound wailing lurches carelessness all-night opportunity. precisely confirmation. warm wallet sturdy swaying alleys pearl privy \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6, Loss: 50.0559\n",
            "  torpedoes dressing-gowns coast packing acre tangle staring forgot he affair stoop pitiful contrast regal shouldered \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11, Loss: 49.9642\n",
            "  wholesale chiefly remarked annoy montenegro massage life.myrtle hadn certainly ballooned tumultuous four intermittent hors-d threadbare \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16, Loss: 49.9583\n",
            "  innocently depression. be drowsiness satisfactory sulkily quick lie. blue san beaver constantly meet expressive mile \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 21, Loss: 49.9524\n",
            "  brewer gravely dress—and contemptuously black weekbe last haven. ’ thanks. sundials boredom who abstracted nope. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 26, Loss: 49.9463\n",
            "  hint coast bureau—gatsby mediterranean—then police different all—except [ moan. heat dipped eberhardt reasons it—i salon \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 31, Loss: 49.9391\n",
            "  clouds underhand crowd. rumored expression highest villainous punch begged garage. spasms houses time— notice yolks \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 36, Loss: 49.9344\n",
            "  rising decline camp asserted room.the hat-boxes mind.jordan montenegro murmur carraways self-consciously newspapers—a ponies reassuring police \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 41, Loss: 49.9047\n",
            "  chair. series floated big grateful frogs dollars.i the than crepe-de-chine , s how unequally d \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 46, Loss: 49.8760\n",
            "  jay. deauville polisher thought speech started—it waiting porch.he impenetrable inquired. dakota wreaths stalled lyric pleasantly \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "\n",
        "    def __init__(self, text, seq_len):\n",
        "        self.text = text\n",
        "        self.seq_len = seq_len\n",
        "        self.tokens = list(set(text)) + [' ', '']\n",
        "        self.token_to_id = {token: idx for idx, token in enumerate(self.tokens)}\n",
        "        self.id_to_token = {idx: token for idx, token in enumerate(self.tokens)}\n",
        "        self.num_tokens = len(self.tokens)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text) // self.seq_len\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start_idx = idx * self.seq_len\n",
        "        end_idx = start_idx + self.seq_len + 1\n",
        "        text_str = self.text[start_idx:end_idx]\n",
        "        text_encoded = [self.token_to_id[token] for token in text_str]\n",
        "        x = torch.tensor(text_encoded[:-1])\n",
        "        y = torch.tensor(text_encoded[1:])\n",
        "        return x, y\n",
        "\n",
        "    def decode(self, text_encoded):\n",
        "        out = ''\n",
        "        for idx in text_encoded:\n",
        "            if int(idx) in self.id_to_token.keys():\n",
        "                out += self.id_to_token[int(idx)]\n",
        "\n",
        "            out += ' '\n",
        "\n",
        "        return out\n",
        "\n",
        "    def encode(self, text):\n",
        "        out = []\n",
        "        for token in text:\n",
        "            if token in self.token_to_id:\n",
        "                out.append(self.token_to_id[token])\n",
        "            else:\n",
        "                out.append(self.token_to_id[' '])\n",
        "        return torch.tensor(out)\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        x = torch.stack([item[0] for item in batch])\n",
        "        y = torch.stack([item[1] for item in batch])\n",
        "        return x, y\n",
        "\n",
        "    def generate_batch(self, batch_size, seq_len, device='cpu'):\n",
        "        while True:\n",
        "            x = torch.randint(self.num_tokens, (batch_size, seq_len), dtype=torch.long, device=device)\n",
        "            y = torch.randint(self.num_tokens, (batch_size, seq_len), dtype=torch.long, device=device)\n",
        "            yield x, y\n",
        "\n",
        "\n",
        "class RNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, num_tokens, emb_size, rnn_num_units, num_layers=1, dropout=0.5):\n",
        "        super().__init__()\n",
        "        self.emb = torch.nn.Embedding(num_tokens, emb_size)\n",
        "        self.rnn = torch.nn.LSTM(emb_size, rnn_num_units, num_layers=num_layers, dropout=dropout)\n",
        "        self.hid_to_logits = torch.nn.Linear(rnn_num_units, num_tokens)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.emb(x)\n",
        "        x, _ = self.rnn(x)\n",
        "        x = self.hid_to_logits(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def generate_sample(model, dataset, prime_str=' ', sample_len=100):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        x = dataset.encode(prime_str)\n",
        "        x = x[None, :].to(next(model.parameters()).device)\n",
        "        for _ in range(sample_len):\n",
        "            logits = model(x)\n",
        "            p_next = torch.nn.functional.softmax(logits[:, -1], dim=-1)\n",
        "            next_token = torch.multinomial(p_next, num_samples=1)\n",
        "            x = torch.cat([x, next_token], dim=1)\n",
        "        return dataset.decode(x[0].cpu())\n",
        "\n",
        "\n",
        "def train(model, dataset, num_epochs, batch_size, lr=1e-3, grad_clip=5, device='cpu'):\n",
        "\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    loader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        for x, y in tqdm(loader, leave=False):\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits.transpose(1, 2), y)\n",
        "            loss.backward()\n",
        "            epoch_loss += loss.item()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            print(f'Epoch: {epoch + 1}, Loss: {epoch_loss:.4f}')\n",
        "            print(generate_sample(model, dataset, sample_len=dataset.seq_len))\n",
        "\n",
        "\n",
        "dataset = TextDataset(text, seq_len=15)\n",
        "model = RNN(num_tokens=dataset.num_tokens, emb_size=256, rnn_num_units=256, num_layers=3, dropout=0.25)\n",
        "train(model, dataset, num_epochs=50, batch_size=512)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71Ni3kSRIo4B",
        "outputId": "c440375a-733b-44d7-8364-0eb1e38114b9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [],
      "metadata": {
        "id": "UcYh0BkDIo4C"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}